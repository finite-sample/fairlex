{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9eebcc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ COMPREHENSIVE HUNGARIAN vs LEXIMIN ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "SCENARIO: Balanced\n",
      "======================================================================\n",
      "==========================================================================================\n",
      "COMPREHENSIVE MATCHING COMPARISON: Hungarian vs Leximin\n",
      "==========================================================================================\n",
      "\n",
      "ðŸ“Š 1-TO-1 MATCHING RESULTS\n",
      "----------------------------------------------------------------------\n",
      "True ATT: 2.0000\n",
      "\n",
      "Method                    ATT Est  Bias     Balance  Max Dist   Time(s) \n",
      "----------------------------------------------------------------------\n",
      "Hungarian 1-to-1          1.5895   -0.4105  0.4021   2.0027     0.0014  \n",
      "Leximin 1-to-1 (Bottleneck) 1.7512   -0.2488  0.5917   2.0027     0.0312  \n",
      "Leximin 1-to-1 (Integer)  1.7512   -0.2488  0.5917   2.0027     0.0283  \n",
      "\n",
      "ðŸ“Š 1-TO-K MATCHING RESULTS\n",
      "----------------------------------------------------------------------\n",
      "Method                         ATT Est  Bias     Balance  Max Dist   Time(s) \n",
      "----------------------------------------------------------------------\n",
      "Hungarian Sequential 1-to-3    1.7982   -0.2018  0.3906   3.8734     0.0007  \n",
      "Hungarian Duplication 1-to-3   1.7982   -0.2018  0.3946   3.1331     0.0010  \n",
      "Hungarian Flow 1-to-3          1.7982   -0.2018  0.3946   3.1331     0.2112  \n",
      "Leximin 1-to-3                 1.7982   -0.2018  0.5020   3.0542     0.0720  \n",
      "\n",
      "ðŸŽ¯ KEY INSIGHTS: Hungarian vs Leximin\n",
      "----------------------------------------------------------------------\n",
      "1-to-1 Max Distance:\n",
      "  Hungarian: 2.0027\n",
      "  Leximin:   2.0027\n",
      "  Improvement: 0.0000 (0.0%)\n",
      "\n",
      "1-to-k ATT Bias (absolute):\n",
      "  Best Hungarian: 0.2018\n",
      "  Leximin:        0.2018\n",
      "\n",
      "======================================================================\n",
      "SCENARIO: Clustered\n",
      "======================================================================\n",
      "==========================================================================================\n",
      "COMPREHENSIVE MATCHING COMPARISON: Hungarian vs Leximin\n",
      "==========================================================================================\n",
      "\n",
      "ðŸ“Š 1-TO-1 MATCHING RESULTS\n",
      "----------------------------------------------------------------------\n",
      "True ATT: 2.0000\n",
      "\n",
      "Method                    ATT Est  Bias     Balance  Max Dist   Time(s) \n",
      "----------------------------------------------------------------------\n",
      "Hungarian 1-to-1          2.3308   0.3308   0.4541   2.0452     0.0001  \n",
      "Leximin 1-to-1 (Bottleneck) 2.3463   0.3463   0.5419   1.6436     0.0225  \n",
      "Leximin 1-to-1 (Integer)  2.3463   0.3463   0.5419   1.6436     0.0206  \n",
      "\n",
      "ðŸ“Š 1-TO-K MATCHING RESULTS\n",
      "----------------------------------------------------------------------\n",
      "Method                         ATT Est  Bias     Balance  Max Dist   Time(s) \n",
      "----------------------------------------------------------------------\n",
      "Hungarian Sequential 1-to-3    2.8842   0.8842   0.6977   4.2832     0.0003  \n",
      "Hungarian Duplication 1-to-3   2.8842   0.8842   0.7081   4.2279     0.0015  \n",
      "Hungarian Flow 1-to-3          2.8842   0.8842   0.7081   4.2279     0.3100  \n",
      "Leximin 1-to-3                 2.8842   0.8842   0.6936   3.7643     0.0620  \n",
      "\n",
      "ðŸŽ¯ KEY INSIGHTS: Hungarian vs Leximin\n",
      "----------------------------------------------------------------------\n",
      "1-to-1 Max Distance:\n",
      "  Hungarian: 2.0452\n",
      "  Leximin:   1.6436\n",
      "  Improvement: 0.4016 (19.6%)\n",
      "\n",
      "1-to-k ATT Bias (absolute):\n",
      "  Best Hungarian: 0.8842\n",
      "  Leximin:        0.8842\n",
      "\n",
      "======================================================================\n",
      "SCENARIO: Sparse\n",
      "======================================================================\n",
      "==========================================================================================\n",
      "COMPREHENSIVE MATCHING COMPARISON: Hungarian vs Leximin\n",
      "==========================================================================================\n",
      "\n",
      "ðŸ“Š 1-TO-1 MATCHING RESULTS\n",
      "----------------------------------------------------------------------\n",
      "True ATT: 2.0000\n",
      "\n",
      "Method                    ATT Est  Bias     Balance  Max Dist   Time(s) \n",
      "----------------------------------------------------------------------\n",
      "Hungarian 1-to-1          2.3304   0.3304   0.5475   5.2427     0.0000  \n",
      "Leximin 1-to-1 (Bottleneck) 1.9856   -0.0144  1.1105   5.0121     0.1463  \n",
      "Leximin 1-to-1 (Integer)  1.9856   -0.0144  1.1105   5.0121     0.1498  \n",
      "\n",
      "ðŸ“Š 1-TO-K MATCHING RESULTS\n",
      "----------------------------------------------------------------------\n",
      "Method                         ATT Est  Bias     Balance  Max Dist   Time(s) \n",
      "----------------------------------------------------------------------\n",
      "Hungarian Sequential 1-to-3    2.6994   0.6994   0.5560   5.9015     0.0002  \n",
      "Hungarian Duplication 1-to-3   2.6994   0.6994   0.5467   5.9015     0.0006  \n",
      "Hungarian Flow 1-to-3          2.6994   0.6994   0.5467   5.9015     0.1600  \n",
      "Leximin 1-to-3                 2.6994   0.6994   1.0153   5.4807     0.2704  \n",
      "\n",
      "ðŸŽ¯ KEY INSIGHTS: Hungarian vs Leximin\n",
      "----------------------------------------------------------------------\n",
      "1-to-1 Max Distance:\n",
      "  Hungarian: 5.2427\n",
      "  Leximin:   5.0121\n",
      "  Improvement: 0.2306 (4.4%)\n",
      "\n",
      "1-to-k ATT Bias (absolute):\n",
      "  Best Hungarian: 0.6994\n",
      "  Leximin:        0.6994\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import linear_sum_assignment, milp, LinearConstraint, Bounds\n",
    "import networkx as nx\n",
    "import time\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# -----------------------------\n",
    "# Fixed Leximin Implementation \n",
    "# -----------------------------\n",
    "\n",
    "def leximin_bottleneck_assignment(dist_matrix):\n",
    "    \"\"\"\n",
    "    Fixed leximin using bottleneck assignment.\n",
    "    This is the correct approach for most matching problems.\n",
    "    \"\"\"\n",
    "    n_treated, n_control = dist_matrix.shape\n",
    "    distances = np.unique(dist_matrix.flatten())\n",
    "    \n",
    "    # Binary search for minimum bottleneck\n",
    "    for threshold in sorted(distances):\n",
    "        binary_matrix = (dist_matrix <= threshold).astype(float)\n",
    "        \n",
    "        try:\n",
    "            row_ind, col_ind = linear_sum_assignment(-binary_matrix)\n",
    "            if np.all(binary_matrix[row_ind, col_ind] == 1):\n",
    "                actual_dists = dist_matrix[row_ind, col_ind]\n",
    "                return col_ind, actual_dists.max()\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Fallback to Hungarian if bottleneck fails\n",
    "    row_ind, col_ind = linear_sum_assignment(dist_matrix)\n",
    "    return col_ind, dist_matrix[row_ind, col_ind].max()\n",
    "\n",
    "def leximin_integer_programming(dist_matrix):\n",
    "    \"\"\"\n",
    "    True leximin using integer programming (for small problems).\n",
    "    \"\"\"\n",
    "    n_treated, n_control = dist_matrix.shape\n",
    "    \n",
    "    if n_treated > 20:  # Too large for integer programming\n",
    "        return leximin_bottleneck_assignment(dist_matrix)\n",
    "    \n",
    "    try:\n",
    "        # Variables: x[i,j] binary + z continuous\n",
    "        n_vars = n_treated * n_control + 1\n",
    "        \n",
    "        # Objective: minimize z\n",
    "        c = np.zeros(n_vars)\n",
    "        c[-1] = 1  # Minimize z\n",
    "        \n",
    "        # Equality constraints: sum_j x[i,j] = 1 for each i\n",
    "        A_eq = np.zeros((n_treated, n_vars))\n",
    "        for i in range(n_treated):\n",
    "            for j in range(n_control):\n",
    "                A_eq[i, i * n_control + j] = 1\n",
    "        b_eq = np.ones(n_treated)\n",
    "        \n",
    "        # Inequality constraints: dist[i,j] * x[i,j] <= z\n",
    "        # Reformulated as: dist[i,j] * x[i,j] - z <= 0\n",
    "        A_ub = np.zeros((n_treated * n_control, n_vars))\n",
    "        for i in range(n_treated):\n",
    "            for j in range(n_control):\n",
    "                idx = i * n_control + j\n",
    "                A_ub[idx, idx] = dist_matrix[i, j]  # dist[i,j] * x[i,j]\n",
    "                A_ub[idx, -1] = -1  # -z\n",
    "        b_ub = np.zeros(n_treated * n_control)\n",
    "        \n",
    "        # Bounds: x[i,j] in {0,1}, z >= 0\n",
    "        bounds = Bounds(\n",
    "            lb=np.zeros(n_vars),\n",
    "            ub=np.concatenate([np.ones(n_treated * n_control), [np.inf]])\n",
    "        )\n",
    "        \n",
    "        # Integer constraints: all x variables are binary\n",
    "        integrality = np.ones(n_vars, dtype=int)\n",
    "        integrality[-1] = 0  # z is continuous\n",
    "        \n",
    "        # Solve\n",
    "        result = milp(\n",
    "            c=c,\n",
    "            integrality=integrality,\n",
    "            A_ub=A_ub, b_ub=b_ub,\n",
    "            A_eq=A_eq, b_eq=b_eq,\n",
    "            bounds=bounds\n",
    "        )\n",
    "        \n",
    "        if result.success:\n",
    "            x_vals = result.x[:-1].reshape(n_treated, n_control)\n",
    "            matches = np.argmax(x_vals, axis=1)\n",
    "            max_dist = result.x[-1]\n",
    "            return matches, max_dist\n",
    "        else:\n",
    "            return leximin_bottleneck_assignment(dist_matrix)\n",
    "            \n",
    "    except Exception:\n",
    "        return leximin_bottleneck_assignment(dist_matrix)\n",
    "\n",
    "# -----------------------------\n",
    "# Hungarian Methods (from earlier)\n",
    "# -----------------------------\n",
    "\n",
    "def hungarian_1to1(dist_matrix):\n",
    "    \"\"\"Standard Hungarian algorithm.\"\"\"\n",
    "    row_ind, col_ind = linear_sum_assignment(dist_matrix)\n",
    "    return col_ind\n",
    "\n",
    "def hungarian_sequential(dist_matrix, k=3):\n",
    "    \"\"\"Sequential Hungarian for 1-to-k matching.\"\"\"\n",
    "    n_treated, n_control = dist_matrix.shape\n",
    "    \n",
    "    if k > n_control:\n",
    "        k = n_control\n",
    "    \n",
    "    matches = {i: [] for i in range(n_treated)}\n",
    "    available_controls = set(range(n_control))\n",
    "    \n",
    "    for round_num in range(k):\n",
    "        if not available_controls:\n",
    "            break\n",
    "        \n",
    "        available_list = sorted(available_controls)\n",
    "        reduced_matrix = dist_matrix[:, available_list]\n",
    "        \n",
    "        row_indices, col_indices = linear_sum_assignment(reduced_matrix)\n",
    "        \n",
    "        for treated_idx, reduced_control_idx in zip(row_indices, col_indices):\n",
    "            original_control_idx = available_list[reduced_control_idx]\n",
    "            matches[treated_idx].append(original_control_idx)\n",
    "            available_controls.remove(original_control_idx)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "def hungarian_duplication(dist_matrix, k=3):\n",
    "    \"\"\"Graph duplication method for 1-to-k matching.\"\"\"\n",
    "    n_treated, n_control = dist_matrix.shape\n",
    "    \n",
    "    if k > n_control:\n",
    "        k = n_control\n",
    "    \n",
    "    # Create expanded cost matrix\n",
    "    expanded_cost = np.tile(dist_matrix, (k, 1))\n",
    "    \n",
    "    # Solve Hungarian on expanded problem\n",
    "    row_indices, col_indices = linear_sum_assignment(expanded_cost)\n",
    "    \n",
    "    # Group results by original treated unit\n",
    "    matches = {i: [] for i in range(n_treated)}\n",
    "    for expanded_row, control in zip(row_indices, col_indices):\n",
    "        original_treated = expanded_row % n_treated\n",
    "        matches[original_treated].append(control)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "def hungarian_flow(dist_matrix, k=3):\n",
    "    \"\"\"Min-cost max-flow for 1-to-k matching.\"\"\"\n",
    "    n_treated, n_control = dist_matrix.shape\n",
    "    \n",
    "    G = nx.DiGraph()\n",
    "    source, sink = \"s\", \"t\"\n",
    "    \n",
    "    for i in range(n_treated):\n",
    "        G.add_edge(source, f\"T{i}\", capacity=k, weight=0)\n",
    "    \n",
    "    for j in range(n_control):\n",
    "        G.add_edge(f\"C{j}\", sink, capacity=1, weight=0)\n",
    "    \n",
    "    for i in range(n_treated):\n",
    "        for j in range(n_control):\n",
    "            dist = dist_matrix[i, j]\n",
    "            G.add_edge(f\"T{i}\", f\"C{j}\", capacity=1, weight=int(dist * 1e6))\n",
    "    \n",
    "    flow = nx.max_flow_min_cost(G, source, sink)\n",
    "    \n",
    "    matches = {i: [] for i in range(n_treated)}\n",
    "    for i in range(n_treated):\n",
    "        for j in range(n_control):\n",
    "            if flow[f\"T{i}\"].get(f\"C{j}\", 0) > 0:\n",
    "                matches[i].append(j)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "def leximin_1tok(dist_matrix, k=3):\n",
    "    \"\"\"\n",
    "    Leximin for 1-to-k matching.\n",
    "    For now, just use k independent bottleneck assignments.\n",
    "    \"\"\"\n",
    "    n_treated, n_control = dist_matrix.shape\n",
    "    \n",
    "    matches = {i: [] for i in range(n_treated)}\n",
    "    available_controls = set(range(n_control))\n",
    "    \n",
    "    for round_num in range(k):\n",
    "        if not available_controls:\n",
    "            break\n",
    "        \n",
    "        available_list = sorted(available_controls)\n",
    "        reduced_matrix = dist_matrix[:, available_list]\n",
    "        \n",
    "        # Use bottleneck assignment for this round\n",
    "        round_matches, _ = leximin_bottleneck_assignment(reduced_matrix)\n",
    "        \n",
    "        for treated_idx, reduced_control_idx in enumerate(round_matches):\n",
    "            if reduced_control_idx < len(available_list):\n",
    "                original_control_idx = available_list[reduced_control_idx]\n",
    "                matches[treated_idx].append(original_control_idx)\n",
    "                if original_control_idx in available_controls:\n",
    "                    available_controls.remove(original_control_idx)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "# -----------------------------\n",
    "# Enhanced Data Generator\n",
    "# -----------------------------\n",
    "def generate_matching_data(n_treated=50, n_control=150, p=5, tau=2.0, \n",
    "                          hetero=False, noise_level=1.0, confounding_strength=0.5, \n",
    "                          seed=42, imbalance_type=\"none\"):\n",
    "    \"\"\"Enhanced data generator with various imbalance patterns.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate covariates based on imbalance type\n",
    "    if imbalance_type == \"clustered\":\n",
    "        # Create clusters to force difficult matching\n",
    "        X_treated = np.random.normal([1, 1, 0, 0, 0], 0.5, (n_treated, p))\n",
    "        X_control = np.random.normal([0, 0, 0, 0, 0], 1.0, (n_control, p))\n",
    "    elif imbalance_type == \"sparse\":\n",
    "        # Some treated units are very isolated\n",
    "        X_treated = np.random.normal(0, 1, (n_treated, p))\n",
    "        X_control = np.random.normal(0, 1, (n_control, p))\n",
    "        # Make 10% of treated units outliers\n",
    "        outlier_count = n_treated // 10\n",
    "        X_treated[-outlier_count:] += np.random.normal(3, 0.5, (outlier_count, p))\n",
    "    else:  # \"none\"\n",
    "        X_treated = np.random.normal(0, 1, (n_treated, p))\n",
    "        X_control = np.random.normal(0, 1, (n_control, p))\n",
    "    \n",
    "    X = np.vstack([X_treated, X_control])\n",
    "    T = np.concatenate([np.ones(n_treated), np.zeros(n_control)])\n",
    "    \n",
    "    # Potential outcomes with confounding\n",
    "    beta = np.random.normal(confounding_strength, 0.1, p)\n",
    "    Y0 = X @ beta + np.random.normal(0, noise_level, len(X))\n",
    "    \n",
    "    # Treatment effect\n",
    "    if hetero:\n",
    "        tau_x = tau * (1 + 0.5 * X[:, 0])\n",
    "    else:\n",
    "        tau_x = np.full(len(X), tau)\n",
    "    \n",
    "    Y1 = Y0 + tau_x\n",
    "    Y = T * Y1 + (1 - T) * Y0\n",
    "    \n",
    "    return X, T, Y, tau_x\n",
    "\n",
    "# -----------------------------\n",
    "# Comprehensive Evaluation\n",
    "# -----------------------------\n",
    "def evaluate_matching_method(X, T, Y, matches, treated_idx, control_idx, tau_x, method_name):\n",
    "    \"\"\"Comprehensive evaluation of any matching method.\"\"\"\n",
    "    results = {'method': method_name}\n",
    "    n_treated = len(treated_idx)\n",
    "    \n",
    "    # Handle both 1-to-1 and 1-to-k matches\n",
    "    if isinstance(matches, dict):\n",
    "        # 1-to-k matching\n",
    "        att_list = []\n",
    "        balance_diffs = []\n",
    "        distances = []\n",
    "        \n",
    "        for i, matched_controls in matches.items():\n",
    "            if matched_controls:\n",
    "                treated_y = Y[treated_idx[i]]\n",
    "                matched_y = Y[control_idx][matched_controls].mean()\n",
    "                att_list.append(treated_y - matched_y)\n",
    "                \n",
    "                treated_x = X[treated_idx[i]]\n",
    "                matched_x = X[control_idx][matched_controls].mean(axis=0)\n",
    "                balance_diffs.append(treated_x - matched_x)\n",
    "                \n",
    "                for j in matched_controls:\n",
    "                    dist = np.linalg.norm(treated_x - X[control_idx[j]])\n",
    "                    distances.append(dist)\n",
    "        \n",
    "        results['avg_matches_per_treated'] = np.mean([len(m) for m in matches.values()])\n",
    "        \n",
    "    else:\n",
    "        # 1-to-1 matching\n",
    "        att_list = []\n",
    "        balance_diffs = []\n",
    "        distances = []\n",
    "        \n",
    "        for i, j in enumerate(matches):\n",
    "            treated_y = Y[treated_idx[i]]\n",
    "            control_y = Y[control_idx[j]]\n",
    "            att_list.append(treated_y - control_y)\n",
    "            \n",
    "            treated_x = X[treated_idx[i]]\n",
    "            control_x = X[control_idx[j]]\n",
    "            balance_diffs.append(treated_x - control_x)\n",
    "            \n",
    "            distances.append(np.linalg.norm(treated_x - control_x))\n",
    "        \n",
    "        results['avg_matches_per_treated'] = 1.0\n",
    "    \n",
    "    # ATT metrics\n",
    "    if att_list:\n",
    "        results['att_estimate'] = np.mean(att_list)\n",
    "        results['att_true'] = np.mean(tau_x[T == 1])\n",
    "        results['att_bias'] = results['att_estimate'] - results['att_true']\n",
    "        results['att_mse'] = results['att_bias'] ** 2\n",
    "    else:\n",
    "        results.update({'att_estimate': np.nan, 'att_true': np.nan, 'att_bias': np.nan, 'att_mse': np.nan})\n",
    "    \n",
    "    # Balance metrics\n",
    "    if balance_diffs:\n",
    "        balance_matrix = np.array(balance_diffs)\n",
    "        results['mean_balance'] = np.abs(balance_matrix).mean()\n",
    "        results['max_balance'] = np.abs(balance_matrix).max()\n",
    "    else:\n",
    "        results['mean_balance'] = np.inf\n",
    "        results['max_balance'] = np.inf\n",
    "    \n",
    "    # Distance metrics\n",
    "    if distances:\n",
    "        results['mean_distance'] = np.mean(distances)\n",
    "        results['max_distance'] = np.max(distances)\n",
    "        results['median_distance'] = np.median(distances)\n",
    "    else:\n",
    "        results['mean_distance'] = np.inf\n",
    "        results['max_distance'] = np.inf\n",
    "        results['median_distance'] = np.inf\n",
    "    \n",
    "    return results\n",
    "\n",
    "# -----------------------------\n",
    "# Complete Comparison Framework\n",
    "# -----------------------------\n",
    "def complete_matching_comparison(X, T, Y, tau_x, k=3, verbose=True):\n",
    "    \"\"\"Compare all matching methods including fixed leximin.\"\"\"\n",
    "    \n",
    "    treated_idx = np.where(T == 1)[0]\n",
    "    control_idx = np.where(T == 0)[0]\n",
    "    \n",
    "    # Calculate distance matrix\n",
    "    dist_matrix = np.linalg.norm(\n",
    "        X[treated_idx][:, None] - X[control_idx][None, :], axis=2\n",
    "    )\n",
    "    \n",
    "    methods = {}\n",
    "    results = []\n",
    "    \n",
    "    # 1-to-1 methods\n",
    "    one_to_one_methods = {\n",
    "        'Hungarian 1-to-1': lambda: hungarian_1to1(dist_matrix),\n",
    "        'Leximin 1-to-1 (Bottleneck)': lambda: leximin_bottleneck_assignment(dist_matrix)[0],\n",
    "        'Leximin 1-to-1 (Integer)': lambda: leximin_integer_programming(dist_matrix)[0],\n",
    "    }\n",
    "    \n",
    "    # 1-to-k methods  \n",
    "    one_to_k_methods = {\n",
    "        f'Hungarian Sequential 1-to-{k}': lambda: hungarian_sequential(dist_matrix, k),\n",
    "        f'Hungarian Duplication 1-to-{k}': lambda: hungarian_duplication(dist_matrix, k),\n",
    "        f'Hungarian Flow 1-to-{k}': lambda: hungarian_flow(dist_matrix, k),\n",
    "        f'Leximin 1-to-{k}': lambda: leximin_1tok(dist_matrix, k),\n",
    "    }\n",
    "    \n",
    "    all_methods = {**one_to_one_methods, **one_to_k_methods}\n",
    "    \n",
    "    for method_name, method_func in all_methods.items():\n",
    "        if verbose:\n",
    "            print(f\"Running {method_name}...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            matches = method_func()\n",
    "            runtime = time.time() - start_time\n",
    "            \n",
    "            eval_results = evaluate_matching_method(\n",
    "                X, T, Y, matches, treated_idx, control_idx, tau_x, method_name\n",
    "            )\n",
    "            eval_results['runtime'] = runtime\n",
    "            eval_results['success'] = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"  Error: {str(e)}\")\n",
    "            eval_results = {\n",
    "                'method': method_name,\n",
    "                'runtime': time.time() - start_time,\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "        \n",
    "        results.append(eval_results)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# -----------------------------\n",
    "# Results Display\n",
    "# -----------------------------\n",
    "def display_comprehensive_results(df_results):\n",
    "    \"\"\"Display comprehensive results with focus on Hungarian vs Leximin.\"\"\"\n",
    "    \n",
    "    print(\"=\" * 90)\n",
    "    print(\"COMPREHENSIVE MATCHING COMPARISON: Hungarian vs Leximin\")\n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    successful = df_results[df_results['success'] == True].copy()\n",
    "    \n",
    "    if len(successful) == 0:\n",
    "        print(\"No methods completed successfully!\")\n",
    "        return\n",
    "    \n",
    "    # Separate 1-to-1 and 1-to-k results\n",
    "    one_to_one = successful[successful['avg_matches_per_treated'] == 1.0]\n",
    "    one_to_k = successful[successful['avg_matches_per_treated'] > 1.0]\n",
    "    \n",
    "    print(f\"\\nðŸ“Š 1-TO-1 MATCHING RESULTS\")\n",
    "    print(\"-\" * 70)\n",
    "    true_att = successful['att_true'].iloc[0]\n",
    "    print(f\"True ATT: {true_att:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    if len(one_to_one) > 0:\n",
    "        print(f\"{'Method':<25} {'ATT Est':<8} {'Bias':<8} {'Balance':<8} {'Max Dist':<10} {'Time(s)':<8}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for _, row in one_to_one.iterrows():\n",
    "            bias_pct = 100 * abs(row['att_bias']) / abs(true_att) if true_att != 0 else 0\n",
    "            print(f\"{row['method']:<25} \"\n",
    "                  f\"{row['att_estimate']:<8.4f} \"\n",
    "                  f\"{row['att_bias']:<8.4f} \"\n",
    "                  f\"{row['mean_balance']:<8.4f} \"\n",
    "                  f\"{row['max_distance']:<10.4f} \"\n",
    "                  f\"{row['runtime']:<8.4f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š 1-TO-K MATCHING RESULTS\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    if len(one_to_k) > 0:\n",
    "        print(f\"{'Method':<30} {'ATT Est':<8} {'Bias':<8} {'Balance':<8} {'Max Dist':<10} {'Time(s)':<8}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for _, row in one_to_k.iterrows():\n",
    "            print(f\"{row['method']:<30} \"\n",
    "                  f\"{row['att_estimate']:<8.4f} \"\n",
    "                  f\"{row['att_bias']:<8.4f} \"\n",
    "                  f\"{row['mean_balance']:<8.4f} \"\n",
    "                  f\"{row['max_distance']:<10.4f} \"\n",
    "                  f\"{row['runtime']:<8.4f}\")\n",
    "    \n",
    "    # Key comparisons\n",
    "    print(f\"\\nðŸŽ¯ KEY INSIGHTS: Hungarian vs Leximin\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Compare 1-to-1 methods\n",
    "    if len(one_to_one) >= 2:\n",
    "        hungarian_1to1 = one_to_one[one_to_one['method'].str.contains('Hungarian 1-to-1')]\n",
    "        leximin_1to1 = one_to_one[one_to_one['method'].str.contains('Leximin')]\n",
    "        \n",
    "        if len(hungarian_1to1) > 0 and len(leximin_1to1) > 0:\n",
    "            h_max = hungarian_1to1['max_distance'].iloc[0]\n",
    "            l_max = leximin_1to1['max_distance'].min()  # Best leximin\n",
    "            \n",
    "            print(f\"1-to-1 Max Distance:\")\n",
    "            print(f\"  Hungarian: {h_max:.4f}\")\n",
    "            print(f\"  Leximin:   {l_max:.4f}\")\n",
    "            print(f\"  Improvement: {h_max - l_max:.4f} ({100*(h_max-l_max)/h_max:.1f}%)\")\n",
    "    \n",
    "    # Compare 1-to-k methods\n",
    "    if len(one_to_k) >= 2:\n",
    "        hungarian_1tok = one_to_k[one_to_k['method'].str.contains('Hungarian')]\n",
    "        leximin_1tok = one_to_k[one_to_k['method'].str.contains('Leximin')]\n",
    "        \n",
    "        if len(hungarian_1tok) > 0 and len(leximin_1tok) > 0:\n",
    "            h_bias = hungarian_1tok['att_bias'].abs().min()  # Best Hungarian\n",
    "            l_bias = leximin_1tok['att_bias'].abs().iloc[0]\n",
    "            \n",
    "            print(f\"\\n1-to-k ATT Bias (absolute):\")\n",
    "            print(f\"  Best Hungarian: {h_bias:.4f}\")\n",
    "            print(f\"  Leximin:        {l_bias:.4f}\")\n",
    "            \n",
    "# -----------------------------\n",
    "# Main Analysis\n",
    "# -----------------------------\n",
    "def run_comprehensive_analysis():\n",
    "    \"\"\"Run the complete Hungarian vs Leximin analysis.\"\"\"\n",
    "    \n",
    "    print(\"ðŸš€ COMPREHENSIVE HUNGARIAN vs LEXIMIN ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Test different scenarios\n",
    "    scenarios = [\n",
    "        (\"Balanced\", {\"imbalance_type\": \"none\", \"n_treated\": 40, \"n_control\": 120}),\n",
    "        (\"Clustered\", {\"imbalance_type\": \"clustered\", \"n_treated\": 40, \"n_control\": 120}),\n",
    "        (\"Sparse\", {\"imbalance_type\": \"sparse\", \"n_treated\": 40, \"n_control\": 120}),\n",
    "    ]\n",
    "    \n",
    "    for scenario_name, params in scenarios:\n",
    "        print(f\"\\n\" + \"=\"*70)\n",
    "        print(f\"SCENARIO: {scenario_name}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Generate data\n",
    "        X, T, Y, tau_x = generate_matching_data(**params)\n",
    "        \n",
    "        # Compare methods\n",
    "        results_df = complete_matching_comparison(X, T, Y, tau_x, k=3, verbose=False)\n",
    "        \n",
    "        # Display results\n",
    "        display_comprehensive_results(results_df)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    final_results = run_comprehensive_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986e773e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Data Science)",
   "language": "python",
   "name": "py311ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
