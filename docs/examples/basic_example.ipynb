{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Survey Weight Calibration: A Realistic Example\n",
    "\n",
    "This example demonstrates how to use fairlex for calibrating survey weights in a realistic scenario. We'll simulate a political opinion survey with typical demographic biases and calibrate against US Census benchmarks.\n",
    "\n",
    "## Scenario\n",
    "\n",
    "A polling organization conducted a survey with 200 respondents to gauge public opinion. Like most surveys, the sample has demographic biases:\n",
    "- Over-representation of older, higher-educated respondents\n",
    "- Under-representation of Hispanic and younger demographics  \n",
    "- Geographic skew toward certain regions\n",
    "\n",
    "We'll use leximin calibration to adjust the weights to match known population demographics from the 2023 US Census."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from fairlex import evaluate_solution, leximin_residual, leximin_weight_fair\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Step 1: Create Realistic Survey Data\n",
    "\n",
    "We'll simulate survey respondents with demographic characteristics that exhibit typical survey biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_respondents = 200\n",
    "\n",
    "# Generate biased survey sample\n",
    "# Age groups: 18-29, 30-44, 45-64, 65+\n",
    "age_groups = np.random.choice(\n",
    "    ['18-29', '30-44', '45-64', '65+'],\n",
    "    size=n_respondents,\n",
    "    p=[0.15, 0.20, 0.35, 0.30]  # Skewed toward older respondents\n",
    ")\n",
    "\n",
    "# Gender: Male, Female\n",
    "gender = np.random.choice(\n",
    "    ['Male', 'Female'],\n",
    "    size=n_respondents,\n",
    "    p=[0.48, 0.52]  # Close to population\n",
    ")\n",
    "\n",
    "# Race/Ethnicity\n",
    "race_ethnicity = np.random.choice(\n",
    "    ['White_NH', 'Black', 'Hispanic', 'Asian', 'Other'],\n",
    "    size=n_respondents,\n",
    "    p=[0.70, 0.11, 0.10, 0.06, 0.03]  # Under-representation of Hispanic, over-representation of White\n",
    ")\n",
    "\n",
    "# Education: HS_or_less, Some_college, Bachelor_plus\n",
    "education = np.random.choice(\n",
    "    ['HS_or_less', 'Some_college', 'Bachelor_plus'],\n",
    "    size=n_respondents,\n",
    "    p=[0.25, 0.30, 0.45]  # Over-representation of college educated\n",
    ")\n",
    "\n",
    "# Region: Northeast, Midwest, South, West\n",
    "region = np.random.choice(\n",
    "    ['Northeast', 'Midwest', 'South', 'West'],\n",
    "    size=n_respondents,\n",
    "    p=[0.20, 0.22, 0.35, 0.23]  # Roughly representative\n",
    ")\n",
    "\n",
    "# Create DataFrame\n",
    "survey_data = pd.DataFrame({\n",
    "    'age_group': age_groups,\n",
    "    'gender': gender,\n",
    "    'race_ethnicity': race_ethnicity,\n",
    "    'education': education,\n",
    "    'region': region\n",
    "})\n",
    "\n",
    "print(\"Survey Sample Demographics:\")\n",
    "print(\"===========================\")\n",
    "for col in survey_data.columns:\n",
    "    print(f\"\\n{col.replace('_', ' ').title()}:\")\n",
    "    print(survey_data[col].value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Step 2: Define Population Benchmarks\n",
    "\n",
    "These targets are based on 2023 US Census data and represent the true population distributions we want to match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Population benchmarks from 2023 US Census data\n",
    "population_targets = {\n",
    "    # Age distribution (approximate from Census data)\n",
    "    'age_18_29': 0.18,\n",
    "    'age_30_44': 0.25,\n",
    "    'age_45_64': 0.32,\n",
    "    'age_65_plus': 0.25,\n",
    "\n",
    "    # Gender distribution\n",
    "    'male': 0.495,\n",
    "    'female': 0.505,\n",
    "\n",
    "    # Race/Ethnicity distribution\n",
    "    'white_nh': 0.582,\n",
    "    'black': 0.120,\n",
    "    'hispanic': 0.190,\n",
    "    'asian': 0.058,\n",
    "    'other_race': 0.050,\n",
    "\n",
    "    # Education distribution (adults 25+, approximate)\n",
    "    'hs_or_less': 0.38,\n",
    "    'some_college': 0.28,\n",
    "    'bachelor_plus': 0.34,\n",
    "\n",
    "    # Regional distribution\n",
    "    'northeast': 0.17,\n",
    "    'midwest': 0.21,\n",
    "    'south': 0.38,\n",
    "    'west': 0.24\n",
    "}\n",
    "\n",
    "print(\"Population Targets (2023 US Census):\")\n",
    "print(\"====================================\")\n",
    "for category, target in population_targets.items():\n",
    "    print(f\"{category.replace('_', ' ').title()}: {target:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Step 3: Construct Membership Matrix\n",
    "\n",
    "The membership matrix A defines which respondents belong to each demographic group. Each row represents a demographic category, and each column represents a respondent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create membership matrix A\n",
    "membership_indicators = []\n",
    "target_totals = []\n",
    "margin_labels = []\n",
    "\n",
    "# Age groups\n",
    "for age_group, target_prop in zip(['18-29', '30-44', '45-64', '65+'],\n",
    "                                  [population_targets['age_18_29'], population_targets['age_30_44'],\n",
    "                                   population_targets['age_45_64'], population_targets['age_65_plus']]):\n",
    "    indicator = (survey_data['age_group'] == age_group).astype(float)\n",
    "    membership_indicators.append(indicator)\n",
    "    target_totals.append(target_prop * n_respondents)\n",
    "    margin_labels.append(f'Age {age_group}')\n",
    "\n",
    "# Gender\n",
    "for gender_val, target_prop in zip(['Male', 'Female'],\n",
    "                                   [population_targets['male'], population_targets['female']]):\n",
    "    indicator = (survey_data['gender'] == gender_val).astype(float)\n",
    "    membership_indicators.append(indicator)\n",
    "    target_totals.append(target_prop * n_respondents)\n",
    "    margin_labels.append(f'Gender {gender_val}')\n",
    "\n",
    "# Race/Ethnicity\n",
    "race_mapping = {\n",
    "    'White_NH': population_targets['white_nh'],\n",
    "    'Black': population_targets['black'],\n",
    "    'Hispanic': population_targets['hispanic'],\n",
    "    'Asian': population_targets['asian'],\n",
    "    'Other': population_targets['other_race']\n",
    "}\n",
    "for race_val, target_prop in race_mapping.items():\n",
    "    indicator = (survey_data['race_ethnicity'] == race_val).astype(float)\n",
    "    membership_indicators.append(indicator)\n",
    "    target_totals.append(target_prop * n_respondents)\n",
    "    margin_labels.append(f'Race {race_val}')\n",
    "\n",
    "# Education\n",
    "edu_mapping = {\n",
    "    'HS_or_less': population_targets['hs_or_less'],\n",
    "    'Some_college': population_targets['some_college'],\n",
    "    'Bachelor_plus': population_targets['bachelor_plus']\n",
    "}\n",
    "for edu_val, target_prop in edu_mapping.items():\n",
    "    indicator = (survey_data['education'] == edu_val).astype(float)\n",
    "    membership_indicators.append(indicator)\n",
    "    target_totals.append(target_prop * n_respondents)\n",
    "    margin_labels.append(f'Education {edu_val}')\n",
    "\n",
    "# Region\n",
    "region_mapping = {\n",
    "    'Northeast': population_targets['northeast'],\n",
    "    'Midwest': population_targets['midwest'],\n",
    "    'South': population_targets['south'],\n",
    "    'West': population_targets['west']\n",
    "}\n",
    "for region_val, target_prop in region_mapping.items():\n",
    "    indicator = (survey_data['region'] == region_val).astype(float)\n",
    "    membership_indicators.append(indicator)\n",
    "    target_totals.append(target_prop * n_respondents)\n",
    "    margin_labels.append(f'Region {region_val}')\n",
    "\n",
    "# Population total constraint\n",
    "total_indicator = np.ones(n_respondents)\n",
    "membership_indicators.append(total_indicator)\n",
    "target_totals.append(n_respondents)\n",
    "margin_labels.append('Total Population')\n",
    "\n",
    "# Convert to arrays\n",
    "A = np.array(membership_indicators, dtype=float)\n",
    "b = np.array(target_totals, dtype=float)\n",
    "\n",
    "print(f\"Membership matrix shape: {A.shape}\")\n",
    "print(f\"Number of demographic margins: {len(margin_labels)}\")\n",
    "print(f\"Number of respondents: {n_respondents}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Step 4: Set Up Base Weights\n",
    "\n",
    "We start with equal base weights representing a simple random sample design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base weights (equal weights for simple random sample)\n",
    "w0 = np.ones(n_respondents)\n",
    "\n",
    "print(f\"Base weights: {n_respondents} equal weights of {w0[0]:.1f}\")\n",
    "print(f\"Base weight total: {w0.sum():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Step 5: Analyze Pre-Calibration Bias\n",
    "\n",
    "Let's examine the demographic bias in our sample before calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "# Calculate current sample proportions\ncurrent_totals = A @ w0\ncurrent_props = current_totals / n_respondents\ntarget_props = b / n_respondents\n\n# Create comparison DataFrame\nbias_analysis = pd.DataFrame({\n    'Demographic': margin_labels,\n    'Sample_%': current_props * 100,\n    'Target_%': target_props * 100,\n    'Difference': (current_props - target_props) * 100\n})\n\nprint(\"Pre-Calibration Demographic Bias:\")\nprint(\"==================================\")\nprint(bias_analysis.round(1))\n\n# Highlight largest biases\nlargest_biases = bias_analysis.iloc[:-1].sort_values('Difference', key=abs, ascending=False).head(5)\nprint(\"\\nLargest Demographic Biases:\")\nprint(\"===========================\")\nfor _, row in largest_biases.iterrows():\n    direction = \"over\" if row['Difference'] > 0 else \"under\"\n    print(f\"{row['Demographic']}: {abs(row['Difference']):.1f}pp {direction}-represented\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Step 6: Apply Leximin Calibration\n",
    "\n",
    "We'll apply both calibration methods available in fairlex:\n",
    "1. **Residual leximin**: Minimizes the worst margin error\n",
    "2. **Weight-fair leximin**: Balances margin accuracy with weight stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Residual leximin calibration\n",
    "result_residual = leximin_residual(\n",
    "    A, b, w0,\n",
    "    min_ratio=0.2,  # Allow weights to be as low as 0.2x original\n",
    "    max_ratio=5.0   # Allow weights to be as high as 5.0x original\n",
    ")\n",
    "\n",
    "print(\"Residual Leximin Results:\")\n",
    "print(\"========================\")\n",
    "print(f\"Optimization status: {result_residual.status} ({result_residual.message})\")\n",
    "print(f\"Maximum absolute residual (epsilon): {result_residual.epsilon:.4f}\")\n",
    "print(f\"Weight range: [{result_residual.w.min():.3f}, {result_residual.w.max():.3f}]\")\n",
    "print(f\"Weight mean: {result_residual.w.mean():.3f}\")\n",
    "\n",
    "# Method 2: Weight-fair leximin calibration\n",
    "result_weight_fair = leximin_weight_fair(\n",
    "    A, b, w0,\n",
    "    min_ratio=0.2,\n",
    "    max_ratio=5.0,\n",
    "    slack=0.001  # Allow small additional margin error for better weight stability\n",
    ")\n",
    "\n",
    "print(\"\\nWeight-Fair Leximin Results:\")\n",
    "print(\"============================\")\n",
    "print(f\"Optimization status: {result_weight_fair.status} ({result_weight_fair.message})\")\n",
    "print(f\"Maximum absolute residual (epsilon): {result_weight_fair.epsilon:.4f}\")\n",
    "print(f\"Maximum relative weight change (t): {result_weight_fair.t:.4f}\")\n",
    "print(f\"Weight range: [{result_weight_fair.w.min():.3f}, {result_weight_fair.w.max():.3f}]\")\n",
    "print(f\"Weight mean: {result_weight_fair.w.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate Calibration Quality\n",
    "\n",
    "Let's assess how well each method performed using comprehensive diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both methods\n",
    "metrics_residual = evaluate_solution(A, b, result_residual.w, base_weights=w0)\n",
    "metrics_weight_fair = evaluate_solution(A, b, result_weight_fair.w, base_weights=w0)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': list(metrics_residual.keys()),\n",
    "    'Residual_Method': list(metrics_residual.values()),\n",
    "    'Weight_Fair_Method': list(metrics_weight_fair.values())\n",
    "})\n",
    "\n",
    "print(\"Calibration Method Comparison:\")\n",
    "print(\"=============================\\n\")\n",
    "print(comparison.round(4))\n",
    "\n",
    "# Interpret key metrics\n",
    "print(\"\\n\\nKey Insights:\")\n",
    "print(\"=============\")\n",
    "print(f\"• Margin Accuracy: Residual method achieves max error of {metrics_residual['resid_max_abs']:.4f}\")\n",
    "print(f\"                   Weight-fair method achieves max error of {metrics_weight_fair['resid_max_abs']:.4f}\")\n",
    "print(f\"• Weight Stability: Residual method ESS = {metrics_residual['ESS']:.1f} (design effect = {metrics_residual['deff']:.2f})\")\n",
    "print(f\"                    Weight-fair method ESS = {metrics_weight_fair['ESS']:.1f} (design effect = {metrics_weight_fair['deff']:.2f})\")\n",
    "print(f\"• Weight Changes: Residual method max change = {metrics_residual['max_rel_dev']:.2%}\")\n",
    "print(f\"                  Weight-fair method max change = {metrics_weight_fair['max_rel_dev']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Step 8: Analyze Post-Calibration Demographics\n",
    "\n",
    "Let's verify that our calibration successfully corrected the demographic biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate post-calibration demographics for weight-fair method\n",
    "calibrated_totals = A @ result_weight_fair.w\n",
    "calibrated_props = calibrated_totals / n_respondents\n",
    "\n",
    "# Create final comparison\n",
    "final_comparison = pd.DataFrame({\n",
    "    'Demographic': margin_labels,\n",
    "    'Original_%': (A @ w0 / n_respondents) * 100,\n",
    "    'Target_%': (b / n_respondents) * 100,\n",
    "    'Calibrated_%': calibrated_props * 100,\n",
    "    'Final_Error': abs(calibrated_props - target_props) * 100\n",
    "})\n",
    "\n",
    "print(\"Post-Calibration Results (Weight-Fair Method):\")\n",
    "print(\"===============================================\\n\")\n",
    "print(final_comparison.round(2))\n",
    "\n",
    "# Summary statistics\n",
    "max_error = final_comparison['Final_Error'].iloc[:-1].max()  # Exclude total row\n",
    "mean_error = final_comparison['Final_Error'].iloc[:-1].mean()\n",
    "\n",
    "print(\"\\nCalibration Summary:\")\n",
    "print(\"===================\")\n",
    "print(f\"Maximum demographic error: {max_error:.3f} percentage points\")\n",
    "print(f\"Average demographic error: {mean_error:.3f} percentage points\")\n",
    "print(f\"All margins calibrated within: ±{max_error:.3f}pp of targets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Step 9: Practical Interpretation\n",
    "\n",
    "Understanding what these results mean for survey analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight distribution analysis\n",
    "weight_stats = pd.DataFrame({\n",
    "    'Statistic': ['Min', 'Q25', 'Median', 'Q75', 'Max', 'Mean', 'Std Dev'],\n",
    "    'Original_Weights': [w0.min(), np.percentile(w0, 25), np.median(w0),\n",
    "                         np.percentile(w0, 75), w0.max(), w0.mean(), w0.std()],\n",
    "    'Calibrated_Weights': [result_weight_fair.w.min(), np.percentile(result_weight_fair.w, 25),\n",
    "                           np.median(result_weight_fair.w), np.percentile(result_weight_fair.w, 75),\n",
    "                           result_weight_fair.w.max(), result_weight_fair.w.mean(),\n",
    "                           result_weight_fair.w.std()]\n",
    "})\n",
    "\n",
    "print(\"Weight Distribution Analysis:\")\n",
    "print(\"============================\\n\")\n",
    "print(weight_stats.round(3))\n",
    "\n",
    "print(\"\\n\\nPractical Implications:\")\n",
    "print(\"=======================\")\n",
    "print(f\"1. Survey Representativeness: Calibration corrected {len([x for x in final_comparison['Final_Error'].iloc[:-1] if abs(x) > 0.001])} demographic biases\")\n",
    "print(f\"2. Effective Sample Size: Reduced from {n_respondents} to {metrics_weight_fair['ESS']:.0f} due to weighting\")\n",
    "print(f\"3. Design Effect: {metrics_weight_fair['deff']:.2f} (variance inflation factor)\")\n",
    "print(f\"4. Weight Variability: Coeffient of variation = {result_weight_fair.w.std() / result_weight_fair.w.mean():.3f}\")\n",
    "\n",
    "print(\"\\nMethod Recommendation:\")\n",
    "print(\"=====================\")\n",
    "if metrics_weight_fair['ESS'] > metrics_residual['ESS']:\n",
    "    print(\"✓ Weight-fair method recommended: Better preserves effective sample size\")\n",
    "else:\n",
    "    print(\"✓ Residual method recommended: Achieves better margin accuracy\")\n",
    "\n",
    "print(f\"\\nCalibration achieves population-representative results with {metrics_weight_fair['ESS']:.0f} effective respondents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This example demonstrated realistic survey weight calibration using fairlex:\n",
    "\n",
    "**Key Features Demonstrated:**\n",
    "- Realistic survey biases (age, education, race/ethnicity skews)\n",
    "- Multiple demographic margins (18 categories + total)\n",
    "- US Census population benchmarks\n",
    "- Comparison of residual vs. weight-fair methods\n",
    "- Comprehensive quality assessment\n",
    "\n",
    "**Typical Use Cases:**\n",
    "- Political polling calibration\n",
    "- Market research weight adjustment\n",
    "- Social survey representativeness correction\n",
    "- Post-stratification in complex surveys\n",
    "\n",
    "**Method Selection Guidelines:**\n",
    "- **Residual leximin**: Use when margin accuracy is paramount\n",
    "- **Weight-fair leximin**: Use when both accuracy and weight stability matter\n",
    "- Consider design effect and effective sample size in your choice\n",
    "\n",
    "The leximin approach ensures that no single demographic group bears a disproportionate burden in achieving representativeness, making it particularly suitable for surveys with multiple important demographic targets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}